{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification - Suspicious Transaction Report and Suspicious Activity Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Text Analytics on Suspicious Transaction Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the modern financial world, organizations and institutions generate vast amounts of textual data in the form of transaction reports, customer communications, and regulatory submissions. This data, while rich in information, can be overwhelming to analyze manually. Leveraging computational methods to extract insights becomes not just advantageous but essential. The focus of this project is to conduct text analytics, specifically targeting Suspicious Transaction Reports (STRs), Suspicious Activity Reports (SARs), and Suspicious Transaction and Activity Reports (STARs).\n",
    "\n",
    "The primary objectives of this project are:\n",
    "\n",
    "1. Topic Modeling: To uncover the underlying themes or topics within the reports. By doing so, we aim to provide a concise summary of the primary subjects present in the dataset.\n",
    "\n",
    "2. Text Classification: To categorize reports into pertinent categories, namely Money Laundering (ML), Terrorism Financing (TF), and Proliferation Financing (PF). This classification aids in  streamlining further investigations and actions based on the nature of the suspicious activity.\n",
    "\n",
    "To achieve these objectives, we employ a combination of traditional machine learning techniques, keyword-based labeling, and advanced natural language processing tools. The project offers a blend of exploratory data analysis and predictive modeling, with the ultimate goal of enhancing the efficiency and effectiveness of financial oversight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Assumptions on the Project:*\n",
    "\n",
    "1. Labeling Mechanism: The keywords or rules used to initially label the data (e.g., for Money Laundering, Terrorism Financing, etc.) are adequate and cover most cases.\n",
    "\n",
    "2. Generalizability: The trained text classification model will be able to generalize and categorize new, unseen financial reports correctly.\n",
    "\n",
    "3. Text as Primary Data: The textual content of the reports provides sufficient information for classification and analysis, without needing supplementary structured numeric data. \n",
    "\n",
    "4. Static Environment: The nature and structure of suspicious reports won't drastically change in the near future. This means the model and insights derived now will remain relevant for some time.\n",
    "\n",
    "5. Analysis Goals: The primary goal of the project is to categorize reports and identify underlying topics. Advanced analytics, like predicting future trends or detecting sophisticated financial crimes, might require additional data and methodologies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Scope:*\n",
    "\n",
    "1. Data Understanding and Preparation:\n",
    "\n",
    "Load and preprocess reports for text analytics.\n",
    "Understand the distribution and nature of reports in terms of length and content.\n",
    "\n",
    "2. Topic Discovery:\n",
    "\n",
    "Uncover underlying topics or themes present in the dataset to provide a concise summary of the primary subjects present.\n",
    "\n",
    "3. Text Classification:\n",
    "\n",
    "Categorize reports into pertinent categories, namely Money Laundering (ML), Terrorism Financing (TF), and Proliferation Financing (PF).\n",
    "Train a classifier to streamline further investigations and actions based on the nature of suspicious activities.\n",
    "\n",
    "4. Data Visualization:\n",
    "\n",
    "Create visual representations of the data to provide insights into report distribution, prominent words, and report structure.\n",
    "\n",
    "### *Research Questions:*\n",
    "\n",
    "1. What are the main topics discussed in the financial reports?\n",
    "\n",
    "Can we identify specific themes or subjects that are frequently mentioned in the dataset?\n",
    "\n",
    "2. How are reports distributed across the categories of Money Laundering (ML), Terrorism Financing (TF), and Proliferation Financing (PF)?\n",
    "\n",
    "Are there certain categories that are more prevalent than others in the dataset?\n",
    "\n",
    "3. Can we effectively train a machine learning model to classify new reports into the aforementioned categories?\n",
    "\n",
    "How well does the model generalize to unseen reports?\n",
    "What features or words play a significant role in determining the category of a report?\n",
    "\n",
    "4. What are the common words or phrases associated with each category?\n",
    "\n",
    "Are there specific terminologies or phrases that are indicative of Money Laundering, Terrorism Financing, or Proliferation Financing?\n",
    "\n",
    "5. What is the structure of the reports in terms of length and content?\n",
    "\n",
    "Are most reports detailed and lengthy, or are they concise and to the point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 396.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.3.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/e6/7c/96a44dabe8577f43ac34e34d0ac098ee42390a06fee4cbe8b5317ecf2520/regex-2023.8.8-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp310-cp310-win_amd64.whl.metadata (42 kB)\n",
      "     -------------------------------------- 42.0/42.0 kB 407.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading regex-2023.8.8-cp310-cp310-win_amd64.whl (268 kB)\n",
      "   -------------------------------------- 268.3/268.3 kB 550.3 kB/s eta 0:00:00\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.8.8\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Installing and Loading Libraries\n",
    "\n",
    "# Install libraries (execute this in your local environment)\n",
    "# !pip install pandas\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Obtaining dependency information for wordcloud from https://files.pythonhosted.org/packages/5a/5f/f4164295d6853ec6203bc728dea4da76a7145ba70482eab9e994d26e0e13/wordcloud-1.9.2-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading wordcloud-1.9.2-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordcloud) (1.22.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordcloud) (3.6.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.2-cp310-cp310-win_amd64.whl (152 kB)\n",
      "   -------------------------------------- 152.1/152.1 kB 909.3 kB/s eta 0:00:00\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post9.tar.gz (3.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [18 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      \n",
      "      If the previous advice does not cover your use case, feel free to report it at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "# !pip install wordcloud\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/59/ed/548f6f686845d386a727a51a3daa411d95fc599649a2d54705f6773ac259/scikit_learn-1.3.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.1-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\rick-royal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.1-cp310-cp310-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 9.3/9.3 MB 118.5 kB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.1 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rick-\n",
      "[nltk_data]     Royal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rick-\n",
      "[nltk_data]     Royal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download datasets for nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Loading the .txt File\n",
    "with open(r\"C:\\Users\\Rick-Royal\\Documents\\Strathmore University Data Science and Analytics\\Module 4\\Module-IV\\Dissertation II\\Paper NLP Project\\str.txt\", \"r\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suspicious Transaction Report (STR) - Domestic Scenario\n",
      "Paragraph 1: Profile of Customer/Entity Identification Number: XYZ12345 Tax Registration Number: T123456789 Date of Account Opening: 01/15/20XX Account Number: XXXXXXXX Account Signatories: John Doe, Jane Smith\n",
      "Paragraph 2: Transaction Description A transaction occurred on 08/20/20XX involving a transfer of $50,000 from Account XXXXXXXX to Account YYYYYYYY.\n",
      "Paragraph 3: Suspicious Transaction Upon analysis, it was observed that the transaction occurred in a manner inconsistent with the customer's typical transaction history. The amount transferred is significantly higher than usual, and the beneficiary of the transfer, Account YYYYYYYY, has no apparent connection to the customer.\n",
      "Paragraph 4: Basis of Suspicion The sudden and unexplained deviation from the customer's established transaction pattern raises concerns of possible money laundering through the placement stage. The lack of a clear connection between the customer and the \n"
     ]
    }
   ],
   "source": [
    "print(text_data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Load the content from 'str_2.txt'\n",
    "with open(r\"C:\\Users\\Rick-Royal\\Documents\\Strathmore University Data Science and Analytics\\Module 4\\Module-IV\\Dissertation II\\Paper NLP Project\\str_2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content_latest = file.read()\n",
    "\n",
    "# Step 1: Extracting the text using the specified delimiters '*' for start of report and '^' for end of report\n",
    "pattern_reference = r'\\*(.*?)\\^'\n",
    "matches_reference = re.findall(pattern_reference, content_latest, re.DOTALL)\n",
    "\n",
    "# Step 2: Preparing data for CSV\n",
    "csv_data_reference = []\n",
    "for match in matches_reference:\n",
    "    title, content = match.split(\"#\", 1)\n",
    "    csv_data_reference.append((title.strip(), content.strip()))\n",
    "\n",
    "# Step 3: Saving the data to a CSV\n",
    "csv_path_reference = r\"C:\\Users\\Rick-Royal\\Downloads\\results\\reports_reference.csv\"\n",
    "with open(csv_path_reference, \"w\", encoding=\"utf-8\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Title\", \"Report Content\"])\n",
    "    writer.writerows(csv_data_reference)\n",
    "\n",
    "# Step 4: Saving each report as an individual document in a corpus directory\n",
    "corpus_dir_reference = r\"C:\\Users\\Rick-Royal\\Downloads\\results\\corpus_reference\"\n",
    "os.makedirs(corpus_dir_reference, exist_ok=True)\n",
    "for title, content in csv_data_reference:\n",
    "    filename = title.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "    with open(f\"{corpus_dir_reference}/{filename}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Suspicious Transaction Report (STR) - Domestic...   \n",
      "1  Suspicious Activity Report (SAR) - Cross-Borde...   \n",
      "2  Suspicious Transaction and Activity Report (ST...   \n",
      "3  Suspicious Transaction Report (STR) - Domestic...   \n",
      "4  Suspicious Activity Report (SAR) - Cross-Borde...   \n",
      "\n",
      "                                      Report Content  \n",
      "0  Paragraph 1: Profile of Customer/Entity Identi...  \n",
      "1  Paragraph 1: Profile of Customer/Entity Identi...  \n",
      "2  Paragraph 1: Profile of Customer/Entity Identi...  \n",
      "3  Paragraph 1: Profile of Customer/Entity Identi...  \n",
      "4  Paragraph 1: Profile of Customer/Entity Identi...  \n",
      "\n",
      "Suspicious_Activity_Report_(SAR)___Cross_Border_Forex_Bureau_Scenario.txt:\n",
      "Paragraph 1: Profile of Customer/Entity Identification Number: FXT98765 Tax Registration Number: T123456789 Date of Customer Relationship Initiated: 07/10/20XX\n",
      "Paragraph 2: Suspicious Activity Description A customer visited the forex bureau multiple times over the course of a week, requesting information about exchange rates and transaction limits for various currencies. However, no actual currency exchanges were conducted.\n",
      "Paragraph 3: Suspicious Activity Upon analysis, it became apparent that ...\n",
      "\n",
      "Suspicious_Activity_Report_(SAR)___Cross_Border_Money_Remittance_Provider_Scenario.txt:\n",
      "Paragraph 1: Profile of Customer/Entity Identification Number: MRP98765 Tax Registration Number: T123456789 Date of Customer Relationship Initiated: 03/08/20XX\n",
      "Paragraph 2: Suspicious Activity Description A customer frequently visits the money remittance provider but only inquires about the process, fees, and available destinations for remittances. No actual remittance transactions are conducted.\n",
      "Paragraph 3: Suspicious Activity Upon analysis, it becomes apparent that the customer's behavior is ...\n",
      "\n",
      "Suspicious_Activity_Report_(SAR)___Cross_Border_Scenario.txt:\n",
      "Paragraph 1: Profile of Customer Customer: Green Earth Foundation Business Registration Number: BCR789 Tax Registration Number: T98765 Date of Account Opening: 2017-11-05 Account Number: ACCT23456 Directors: Emily White, David Brown Shareholders: Emily White (70%), David Brown (30%)\n",
      "Paragraph 2: Suspicious Activity Description Activity: Numerous outgoing payments to individuals in countries with known high-risk for corruption and bribery. Dates of Activity: 2023-06-01 to 2023-06-30\n",
      "Paragraph 3: ...\n",
      "\n",
      "Suspicious_Transaction_and_Activity_Report_(STAR)___Agricultural_Exporter_Scenario.txt:\n",
      "1.\tProfile of Customer Customer: Green Fields Exports Business Registration Number: BCR610 Tax Registration Number: T61053 Date of Account Opening: 2019-03-12 Account Number: ACCT61090 Directors: Larry Grain, Sarah Seed Beneficial Owner: Terry Terracotta (70%)\n",
      "2.\tTransaction and Activity Description Transaction Date: 2023-06-20 Transaction Amount: $1,200,000 Transaction Type: Payment for Pesticides Activity: Receipt of large payments from non-agricultural sectors. Dates of Activity: 2023-06-22 t...\n",
      "\n",
      "Suspicious_Transaction_and_Activity_Report_(STAR)___Art_Auction_House_Scenario.txt:\n",
      "1.\tProfile of Customer Customer: Renaissance Arts Auctioneers Business Registration Number: BCR421 Tax Registration Number: T42130 Date of Account Opening: 2021-02-20 Account Number: ACCT42156 Directors: Clara Canvas, Victor Brush Beneficial Owner: Paula Palette (85%)\n",
      "2.\tTransaction and Activity Description Transaction Date: 2023-03-10 Transaction Amount: $3,000,000 Transaction Type: Sale of a Rare Painting Activity: Immediate purchase of multiple lesser-known artworks. Dates of Activity: 2023-0...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "csv_path = r\"C:\\Users\\Rick-Royal\\Downloads\\results\\reports_reference.csv\"\n",
    "df_reports = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df_reports.head())\n",
    "\n",
    "# Step 2: Load the corpus (all reports saved as individual documents)\n",
    "corpus_directory = r\"C:\\Users\\Rick-Royal\\Downloads\\results\\corpus_reference\"\n",
    "file_names = os.listdir(corpus_directory)\n",
    "corpus = {}\n",
    "\n",
    "# Reading each file in the corpus directory and storing its content in a dictionary\n",
    "for file_name in file_names:\n",
    "    with open(os.path.join(corpus_directory, file_name), \"r\", encoding=\"utf-8\") as file:\n",
    "        corpus[file_name] = file.read()\n",
    "\n",
    "# Displaying the content of the first few files in the corpus for verification\n",
    "for file_name, content in list(corpus.items())[:5]:\n",
    "    print(f\"\\n{file_name}:\\n{content[:500]}...\")  # Displaying the first 500 characters of each file for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Report Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suspicious Transaction Report (STR) - Domestic...</td>\n",
       "      <td>Paragraph 1: Profile of Customer/Entity Identi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suspicious Activity Report (SAR) - Cross-Borde...</td>\n",
       "      <td>Paragraph 1: Profile of Customer/Entity Identi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suspicious Transaction and Activity Report (ST...</td>\n",
       "      <td>Paragraph 1: Profile of Customer/Entity Identi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suspicious Transaction Report (STR) - Domestic...</td>\n",
       "      <td>Paragraph 1: Profile of Customer/Entity Identi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suspicious Activity Report (SAR) - Cross-Borde...</td>\n",
       "      <td>Paragraph 1: Profile of Customer/Entity Identi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Suspicious Transaction Report (STR) - Domestic...   \n",
       "1  Suspicious Activity Report (SAR) - Cross-Borde...   \n",
       "2  Suspicious Transaction and Activity Report (ST...   \n",
       "3  Suspicious Transaction Report (STR) - Domestic...   \n",
       "4  Suspicious Activity Report (SAR) - Cross-Borde...   \n",
       "\n",
       "                                      Report Content  \n",
       "0  Paragraph 1: Profile of Customer/Entity Identi...  \n",
       "1  Paragraph 1: Profile of Customer/Entity Identi...  \n",
       "2  Paragraph 1: Profile of Customer/Entity Identi...  \n",
       "3  Paragraph 1: Profile of Customer/Entity Identi...  \n",
       "4  Paragraph 1: Profile of Customer/Entity Identi...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first five rows\n",
    "df_reports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Report Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Suspicious Transaction and Activity Report (ST...</td>\n",
       "      <td>1.\\tProfile of Customer Customer: Northern Inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Suspicious Transaction and Activity Report (ST...</td>\n",
       "      <td>1.\\tProfile of Customer Customer: Raymond Scot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Suspicious Transaction and Activity Report (ST...</td>\n",
       "      <td>1.\\tProfile of Customer Customer: TeleNetwork ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Suspicious Transaction and Activity Report (ST...</td>\n",
       "      <td>1.\\tProfile of Customer Customer: Southern Roa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Suspicious Transaction and Activity Report (ST...</td>\n",
       "      <td>1.\\tProfile of Customer Customer: Linda Fletch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "55  Suspicious Transaction and Activity Report (ST...   \n",
       "56  Suspicious Transaction and Activity Report (ST...   \n",
       "57  Suspicious Transaction and Activity Report (ST...   \n",
       "58  Suspicious Transaction and Activity Report (ST...   \n",
       "59  Suspicious Transaction and Activity Report (ST...   \n",
       "\n",
       "                                       Report Content  \n",
       "55  1.\\tProfile of Customer Customer: Northern Inf...  \n",
       "56  1.\\tProfile of Customer Customer: Raymond Scot...  \n",
       "57  1.\\tProfile of Customer Customer: TeleNetwork ...  \n",
       "58  1.\\tProfile of Customer Customer: Southern Roa...  \n",
       "59  1.\\tProfile of Customer Customer: Linda Fletch...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check last five rows\n",
    "df_reports.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the dataframe\n",
    "df_reports.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title             0\n",
       "Report Content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "df_reports.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of documents in the corpus_directory\n",
    "len(corpus_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Conducting data cleaning\n",
    "\n",
    "# Tokenization\n",
    "sentences = sent_tokenize(text_data)\n",
    "words = word_tokenize(text_data)\n",
    "\n",
    "# Stopword Removal\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Remove punctuation marks\n",
    "punctuation = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', \"%\", '-']\n",
    "filtered_words = [word for word in filtered_words if word not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Suspicious Transaction Report (STR) - Domestic Scenario\\nParagraph 1: Profile of Customer/Entity Identification Number: XYZ12345 Tax Registration Number: T123456789 Date of Account Opening: 01/15/20XX Account Number: XXXXXXXX Account Signatories: John Doe, Jane Smith\\nParagraph 2: Transaction Description A transaction occurred on 08/20/20XX involving a transfer of $50,000 from Account XXXXXXXX to Account YYYYYYYY.',\n",
       " \"Paragraph 3: Suspicious Transaction Upon analysis, it was observed that the transaction occurred in a manner inconsistent with the customer's typical transaction history.\",\n",
       " 'The amount transferred is significantly higher than usual, and the beneficiary of the transfer, Account YYYYYYYY, has no apparent connection to the customer.',\n",
       " \"Paragraph 4: Basis of Suspicion The sudden and unexplained deviation from the customer's established transaction pattern raises concerns of possible money laundering through the placement stage.\",\n",
       " \"The lack of a clear connection between the customer and the beneficiary also heightens suspicions of layering activities aimed at obfuscating the funds' origin.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Suspicious',\n",
       " 'Transaction',\n",
       " 'Report',\n",
       " '(',\n",
       " 'STR',\n",
       " ')',\n",
       " '-',\n",
       " 'Domestic',\n",
       " 'Scenario',\n",
       " 'Paragraph']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Suspicious',\n",
       " 'Transaction',\n",
       " 'Report',\n",
       " 'STR',\n",
       " 'Domestic',\n",
       " 'Scenario',\n",
       " 'Paragraph',\n",
       " '1',\n",
       " 'Profile',\n",
       " 'Customer/Entity']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 406\n",
      "Number of Words: 8814\n",
      "Number of Words after Stopword Removal: 5752\n",
      "Most Common Words: [('Transaction', 254), ('Activity', 163), ('Number', 142), ('Suspicious', 125), ('Account', 91), ('Date', 89), ('Customer', 75), ('Registration', 72), ('Report', 60), ('Paragraph', 60)]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Conducting Text Analytics\n",
    "\n",
    "# Frequency Distribution\n",
    "freq_dist = FreqDist(filtered_words)\n",
    "\n",
    "# Displaying some basic information\n",
    "num_sentences = len(sentences)\n",
    "num_words = len(words)\n",
    "num_filtered_words = len(filtered_words)\n",
    "most_common_words = freq_dist.most_common(10)\n",
    "\n",
    "print(f\"Number of Sentences: {num_sentences}\")\n",
    "print(f\"Number of Words: {num_words}\")\n",
    "print(f\"Number of Words after Stopword Removal: {num_filtered_words}\")\n",
    "print(f\"Most Common Words: {most_common_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documents: 50\n",
      "Average Document Length (in words): 110.90\n",
      "Shortest Document Length (in words): 77\n",
      "Longest Document Length (in words): 170\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load all documents in the corpus\n",
    "corpus_directory = r\"C:\\Users\\Rick-Royal\\Downloads\\results\\corpus_reference\"\n",
    "documents = []\n",
    "file_names = os.listdir(corpus_directory)\n",
    "for file_name in file_names:\n",
    "    with open(os.path.join(corpus_directory, file_name), \"r\", encoding=\"utf-8\") as file:\n",
    "        documents.append(file.read())\n",
    "\n",
    "# 1. Basic Statistics\n",
    "num_docs = len(documents)\n",
    "doc_lengths = [len(doc.split()) for doc in documents]\n",
    "avg_doc_length = sum(doc_lengths) / num_docs\n",
    "min_doc_length = min(doc_lengths)\n",
    "max_doc_length = max(doc_lengths)\n",
    "\n",
    "print(f\"Total Number of Documents: {num_docs}\")\n",
    "print(f\"Average Document Length (in words): {avg_doc_length:.2f}\")\n",
    "print(f\"Shortest Document Length (in words): {min_doc_length}\")\n",
    "print(f\"Longest Document Length (in words): {max_doc_length}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 200 characters of the first 5 documents:\n",
      "\n",
      "Document 1:\n",
      "Paragraph 1: Profile of Customer/Entity Identification Number: FXT98765 Tax Registration Number: T123456789 Date of Customer Relationship Initiated: 07/10/20XX\n",
      "Paragraph 2: Suspicious Activity Descrip...\n",
      "\n",
      "Document 2:\n",
      "Paragraph 1: Profile of Customer/Entity Identification Number: MRP98765 Tax Registration Number: T123456789 Date of Customer Relationship Initiated: 03/08/20XX\n",
      "Paragraph 2: Suspicious Activity Descrip...\n",
      "\n",
      "Document 3:\n",
      "Paragraph 1: Profile of Customer Customer: Green Earth Foundation Business Registration Number: BCR789 Tax Registration Number: T98765 Date of Account Opening: 2017-11-05 Account Number: ACCT23456 Dir...\n",
      "\n",
      "Document 4:\n",
      "1.\tProfile of Customer Customer: Green Fields Exports Business Registration Number: BCR610 Tax Registration Number: T61053 Date of Account Opening: 2019-03-12 Account Number: ACCT61090 Directors: Larr...\n",
      "\n",
      "Document 5:\n",
      "1.\tProfile of Customer Customer: Renaissance Arts Auctioneers Business Registration Number: BCR421 Tax Registration Number: T42130 Date of Account Opening: 2021-02-20 Account Number: ACCT42156 Directo...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Text Exploration\n",
    "print(\"\\nFirst 200 characters of the first 5 documents:\")\n",
    "for i, doc in enumerate(documents[:5]):\n",
    "    print(f\"\\nDocument {i + 1}:\\n{doc[:200]}...\")\n",
    "\n",
    "# Distribution of document lengths\n",
    "plt.hist(doc_lengths, bins=30, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Document Lengths')\n",
    "plt.xlabel('Document Length (in words)')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of the reports had document lengths between 105 and 130 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Common Words:\n",
      "of: 290\n",
      "transaction: 188\n",
      "a: 152\n",
      "to: 146\n",
      "the: 137\n",
      "number:: 112\n",
      "and: 112\n",
      "activity: 65\n",
      "account: 65\n",
      "registration: 60\n",
      "\n",
      "Most Common Bigrams:\n",
      "registration number:: 60\n",
      "transaction and: 60\n",
      "and activity: 58\n",
      "profile of: 50\n",
      "date of: 50\n",
      "basis of: 50\n",
      "of suspicion: 50\n",
      "suspicious transaction: 47\n",
      "1. profile: 41\n",
      "2. transaction: 41\n"
     ]
    }
   ],
   "source": [
    "# 3. Common Words and Phrases\n",
    "all_words = [word.lower() for doc in documents for word in doc.split()]\n",
    "word_freq = Counter(all_words)\n",
    "common_words = word_freq.most_common(10)\n",
    "\n",
    "print(\"\\nMost Common Words:\")\n",
    "for word, freq in common_words:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# Most frequent bigrams\n",
    "bigrams = [(all_words[i], all_words[i + 1]) for i in range(len(all_words) - 1)]\n",
    "bigram_freq = Counter(bigrams)\n",
    "common_bigrams = bigram_freq.most_common(10)\n",
    "\n",
    "print(\"\\nMost Common Bigrams:\")\n",
    "for (word1, word2), freq in common_bigrams:\n",
    "    print(f\"{word1} {word2}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA): It assumes each document is a mix of topics and a topic is a mix of words. LDA tries to map all the documents to the topics in a way such that the words in each document are mostly captured by those imaginary topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic #0: transaction, activity, suspicious, 000, description, type, date, 2023, funds, payment',\n",
       " 'Topic #1: 2023, activity, transaction, paragraph, 000, description, dates, 20xx, 10, customer',\n",
       " 'Topic #2: number, account, customer, registration, profile, date, tax, opening, entity, identification',\n",
       " 'Topic #3: transaction, suspicious, report, scenario, ________________________________________, activity, star, str, risk, jurisdiction',\n",
       " 'Topic #4: suspicion, basis, funds, potential, money, laundering, large, high, layering, raises']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Number of topics\n",
    "n_topics = 5\n",
    "\n",
    "# Preprocess text data: Tokenization and vectorization\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Apply LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_model.fit(dtm)\n",
    "\n",
    "# Function to print top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        topics.append(message)\n",
    "    return topics\n",
    "\n",
    "n_top_words = 10\n",
    "topic_keywords = print_top_words(lda_model, vectorizer.get_feature_names_out(), n_top_words)\n",
    "topic_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((324, 524), (82, 524))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# labeling of data \n",
    "labels = [\"Suspicious Activity\" if \"suspicious\" in sentence.lower() else \"Not Suspicious Activity\" for sentence in sentences]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer_classification = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_train_vec = vectorizer_classification.fit_transform(X_train)\n",
    "X_test_vec = vectorizer_classification.transform(X_test)\n",
    "\n",
    "X_train_vec.shape, X_test_vec.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Suspicious Activity',\n",
       " 'Suspicious Activity',\n",
       " 'Not Suspicious Activity',\n",
       " 'Not Suspicious Activity',\n",
       " 'Not Suspicious Activity']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Training the Naive Bayes classifier\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# # Predicting on the test set\n",
    "# y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "# # Evaluating the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# accuracy, classification_rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Not Suspicious Activity</th>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suspicious Activity</th>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.975610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.972884</td>\n",
       "      <td>0.972884</td>\n",
       "      <td>0.972884</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         precision    recall  f1-score  support\n",
       "Not Suspicious Activity   0.981481  0.981481  0.981481     54.0\n",
       "Suspicious Activity       0.964286  0.964286  0.964286     28.0\n",
       "accuracy                  0.975610       NaN       NaN      NaN\n",
       "macro avg                 0.972884  0.972884  0.972884     82.0\n",
       "weighted avg              0.975610  0.975610  0.975610     82.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Training the Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Convert the classification report to a DataFrame\n",
    "report_df = pd.DataFrame(classification_rep).transpose()\n",
    "\n",
    "# Add accuracy to the DataFrame\n",
    "report_df.loc['accuracy'] = [accuracy, None, None, None]\n",
    "\n",
    "report_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "       Money Laundering       0.00      0.00      0.00         4\n",
      "                  Other       0.93      1.00      0.96        76\n",
      "Proliferation Financing       0.00      0.00      0.00         1\n",
      "    Terrorism Financing       0.00      0.00      0.00         1\n",
      "\n",
      "               accuracy                           0.93        82\n",
      "              macro avg       0.23      0.25      0.24        82\n",
      "           weighted avg       0.86      0.93      0.89        82\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rick-Royal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Rick-Royal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Rick-Royal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Manually labeling data based on keywords\n",
    "def assign_label(sentence):\n",
    "    if \"money laundering\" in sentence.lower():\n",
    "        return \"Money Laundering\"\n",
    "    elif \"terrorism financing\" in sentence.lower():\n",
    "        return \"Terrorism Financing\"\n",
    "    elif \"proliferation financing\" in sentence.lower():\n",
    "        return \"Proliferation Financing\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "labels_classification = [assign_label(sentence) for sentence in sentences]\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels_classification, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_train_vec = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Training a Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "# Evaluating the classifier\n",
    "classification_results = classification_report(y_test, y_pred)\n",
    "print(classification_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrorism Financing (TF)\n"
     ]
    }
   ],
   "source": [
    "def categorize_report(report):\n",
    "    # Define keywords for each category\n",
    "    ml_keywords = [\"money laundering\", \"laundered funds\", \"illegal transfer\"]\n",
    "    tf_keywords = [\"terrorism financing\", \"terrorist funds\", \"terror financing\"]\n",
    "    pf_keywords = [\"proliferation financing\", \"weapons of mass destruction\", \"nuclear financing\"]\n",
    "\n",
    "    # Check the presence of keywords in the report\n",
    "    if any(keyword in report.lower() for keyword in ml_keywords):\n",
    "        return \"Money Laundering (ML)\"\n",
    "    elif any(keyword in report.lower() for keyword in tf_keywords):\n",
    "        return \"Terrorism Financing (TF)\"\n",
    "    elif any(keyword in report.lower() for keyword in pf_keywords):\n",
    "        return \"Proliferation Financing (PF)\"\n",
    "    else:\n",
    "        return \"Uncategorized\"\n",
    "\n",
    "# Test with a sample report\n",
    "sample_report = \"This is a suspicious transaction related to terrorist funds and activities.\"\n",
    "categorization = categorize_report(sample_report)\n",
    "print(categorization)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
